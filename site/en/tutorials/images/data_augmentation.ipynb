{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "data_augmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RachelRamirez/FashionMNIST_DataAugmentation/blob/main/site/en/tutorials/images/data_augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EFY9e5wRn7v"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "pkTRazeVRwDe"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyOckJu6Rs-i"
      },
      "source": [
        "# Data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HEsULqDR7AH"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/images/data_augmentation\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/data_augmentation.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/data_augmentation.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/images/data_augmentation.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxIOE5RnSQtj"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial demonstrates data augmentation: a technique to increase the diversity of your training set by applying random (but realistic) transformations such as image rotation. You will learn how to apply data augmentation in two ways. First, you will use [Keras Preprocessing Layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/). Next, you will use `tf.image`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UxHAqXmSXN5"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2Q5rPenTAJP"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydx3SSoF4wpG"
      },
      "source": [
        "## Download a dataset\n",
        "\n",
        "This tutorial uses the [tf_flowers](https://www.tensorflow.org/datasets/catalog/tf_flowers) dataset. For convenience, download the dataset using [TensorFlow Datasets](https://www.tensorflow.org/datasets). If you would like to learn about others ways of importing data, see the [load images](https://www.tensorflow.org/tutorials/load_data/images) tutorial.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytHhsYmO52zy"
      },
      "source": [
        "(train_ds, val_ds, test_ds), metadata = tfds.load(\n",
        "    'fashion_mnist',\n",
        "    split=['train[:100]', 'train[100:200]', 'train[201:]'],\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjxEJtCwsnmm"
      },
      "source": [
        "The flowers dataset has five classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKwx7vQuspxz",
        "outputId": "7e1bd249-ed5f-483d-cfc3-47248081b5b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "num_classes = metadata.features['label'].num_classes\n",
        "print(num_classes)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZAQW44949uw"
      },
      "source": [
        "Let's retrieve an image from the dataset and use it to demonstrate data augmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXlx1lCr5Bip",
        "outputId": "0686b3d0-1434-4139-ca05-8afc295a44bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "get_label_name = metadata.features['label'].int2str\n",
        "\n",
        "image, label = next(iter(train_ds))\n",
        "_ = plt.imshow(np.reshape(image,(28,28)))\n",
        "_ = plt.title(get_label_name(label))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdJ6XA4q2nqK"
      },
      "source": [
        "## Use Keras preprocessing layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BTGz5AQ9LcD"
      },
      "source": [
        "Note: The [Keras Preprocesing Layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing) introduced in this section are currently experimental."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRMPnfzBB2hw"
      },
      "source": [
        "### Resizing and rescaling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhG7gSWmUMJx"
      },
      "source": [
        "You can use preprocessing layers to [resize](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Resizing) your images to a consistent shape, and to [rescale](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling) pixel values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMM3b85e3yhd"
      },
      "source": [
        "IMG_SIZE = 28\n",
        "\n",
        "resize_and_rescale = tf.keras.Sequential([\n",
        "#  layers.experimental.preprocessing.Resizing(IMG_SIZE, IMG_SIZE),\n",
        "  layers.experimental.preprocessing.Rescaling(1./255)\n",
        "])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z8AV1WgnYNW"
      },
      "source": [
        "Note: the rescaling layer above standardizes pixel values to `[0,1]`. If instead you wanted `[-1,1]`, you would write `Rescaling(1./127.5, offset=-1)`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQiTwsHJDHAD"
      },
      "source": [
        "You can see the result of applying these layers to an image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9OLuR1bC1Pd",
        "outputId": "46f5f5e1-4761-4dd9-94dd-c2e9ae7b2f48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "source": [
        "result = resize_and_rescale(image)\n",
        "_ = plt.imshow(np.reshape(result,(28,28)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxAMg8Zql5lw"
      },
      "source": [
        "You can verify the pixels are in `[0-1]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPTB8IQmSeKM",
        "outputId": "1971aae0-34ad-4628-bb83-1edbf2a89137",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Min and max pixel values:\", result.numpy().min(), result.numpy().max())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min and max pixel values: 0.0 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL6M7fuivAw4"
      },
      "source": [
        "### Data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL4Suj46ScfU"
      },
      "source": [
        "You can use preprocessing layers for data augmentation as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-4PugTE-4sl"
      },
      "source": [
        "Let's create a few preprocessing layers and apply them repeatedly to the same image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svu_5yfa_Jb7"
      },
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        " # layers.experimental.preprocessing.RandomRotation(0.05),\n",
        "  layers.experimental.preprocessing.RandomContrast(0.1),\n",
        "  layers.experimental.preprocessing.RandomZoom(0.1)\n",
        "])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfzEuaNg69iU"
      },
      "source": [
        "# Add the image to a batch\n",
        "image = tf.expand_dims(image, 0)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR4wwi5Q_UZK",
        "outputId": "122d3a28-9208-4d58-d691-4f2c059ef4c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(9):\n",
        "  augmented_image = data_augmentation(image)\n",
        "  ax = plt.subplot(3, 3, i + 1)\n",
        "  plt.imshow(np.reshape(augmented_image[0],(28,28)))\n",
        "  plt.axis(\"off\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 9 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA17pEeS_2_-"
      },
      "source": [
        "There are a variety of preprocessing [layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing) you can use for data augmentation including `layers.RandomContrast`, `layers.RandomCrop`, `layers.RandomZoom`, and others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG5RhIJtE0ng"
      },
      "source": [
        "### Two options to use the preprocessing layers\n",
        "\n",
        "There are two ways you can use these preprocessing layers, with important tradeoffs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxGvUT727Po6"
      },
      "source": [
        "#### Option 1: Make the preprocessing layers part of your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULGJQjP6hHvu"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  resize_and_rescale,\n",
        "#  data_augmentation,\n",
        "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  # Rest of your model\n",
        "])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc6ELneyhJN9"
      },
      "source": [
        "There are two important points to be aware of in this case:\n",
        "\n",
        "* Data augmentation will run on-device, synchronously with the rest of your layers, and benefit from GPU acceleration.\n",
        "\n",
        "* When you export your model using `model.save`, the preprocessing layers will be saved along with the rest of your model. If you later deploy this model, it will automatically standardize images (according to the configuration of your layers). This can save you from the effort of having to reimplement that logic server-side."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syZwDSpiRXZP"
      },
      "source": [
        "Note: Data augmentation is inactive at test time so input images will only be augmented during calls to `model.fit` (not `model.evaluate` or `model.predict`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2X3JTeY_vfv"
      },
      "source": [
        "#### Option 2: Apply the preprocessing layers to your dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1Bt7w5VhVDY"
      },
      "source": [
        "aug_ds = train_ds.map(\n",
        "  lambda x, y: (resize_and_rescale(x, training=True), y))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKqeahG2hVdV"
      },
      "source": [
        "With this approach, you use `Dataset.map` to create a dataset that yields batches of augmented images. In this case:\n",
        "\n",
        "* Data augmentation will happen asynchronously on the CPU, and is non-blocking. You can overlap the training of your model on the GPU with data preprocessing, using `Dataset.prefetch`, shown below.\n",
        "* In this case the prepreprocessing layers will not be exported with the model when you call `model.save`. You will need to attach them to your model before saving it or reimplement them server-side. After training, you can attach the preprocessing layers before export.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgj51k9J7jfc"
      },
      "source": [
        "You can find an example of the first option in the [image classification](https://www.tensorflow.org/tutorials/images/classification) tutorial. Let's demonstrate the second option here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31YwMQdrXKBP"
      },
      "source": [
        "### Apply the preprocessing layers to the datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUgW-2LOGiOT"
      },
      "source": [
        "Configure the train, validation, and test datasets with the preprocessing layers you created above. You will also configure the datasets for performance, using parallel reads and buffered prefetching to yield batches from disk without I/O become blocking. You can learn more dataset performance in the [Better performance with the tf.data API](https://www.tensorflow.org/guide/data_performance) guide. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI7VdyqK767y"
      },
      "source": [
        "Note: data augmentation should only be applied to the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5fGVMqlFxF7"
      },
      "source": [
        "batch_size = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def prepare(ds, shuffle=False, augment=False):\n",
        "  # Resize and rescale all datasets\n",
        "  ds = ds.map(lambda x, y: (resize_and_rescale(x), y), \n",
        "              num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(1000)\n",
        "\n",
        "  # Batch all datasets\n",
        "  ds = ds.batch(batch_size)\n",
        "\n",
        "  # Use data augmentation only on the training set\n",
        "  if augment:\n",
        "    ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), \n",
        "                num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "  # Use buffered prefecting on all datasets\n",
        "  return ds.prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N86SFGMBHcx-"
      },
      "source": [
        "train_ds = prepare(train_ds, shuffle=True, augment=True)\n",
        "val_ds = prepare(val_ds)\n",
        "test_ds = prepare(test_ds)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gplDz4ZV6kk"
      },
      "source": [
        "### Train a model\n",
        "\n",
        "For completeness, you will now train a model using these datasets. This model has not been tuned for accuracy (the goal is to show you the mechanics)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IODSymGhq9N6"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(num_classes)\n",
        "])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnRJr95WY68k"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_sDl9uZY9Mh",
        "outputId": "eda8dff6-fff3-4b75-be08-3ee04eb320f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "epochs=50\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs\n",
        ")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "4/4 [==============================] - 1s 77ms/step - loss: 2.2747 - accuracy: 0.1200 - val_loss: 2.2558 - val_accuracy: 0.1100\n",
            "Epoch 2/50\n",
            "4/4 [==============================] - 0s 42ms/step - loss: 2.1804 - accuracy: 0.1300 - val_loss: 2.1964 - val_accuracy: 0.1600\n",
            "Epoch 3/50\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 2.1014 - accuracy: 0.2100 - val_loss: 2.1518 - val_accuracy: 0.1300\n",
            "Epoch 4/50\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 2.0065 - accuracy: 0.2100 - val_loss: 2.0706 - val_accuracy: 0.2600\n",
            "Epoch 5/50\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 1.8779 - accuracy: 0.3400 - val_loss: 1.9057 - val_accuracy: 0.2900\n",
            "Epoch 6/50\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 1.7601 - accuracy: 0.4600 - val_loss: 1.7311 - val_accuracy: 0.5000\n",
            "Epoch 7/50\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 1.5842 - accuracy: 0.4200 - val_loss: 1.5252 - val_accuracy: 0.5900\n",
            "Epoch 8/50\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 1.4048 - accuracy: 0.5300 - val_loss: 1.4444 - val_accuracy: 0.4700\n",
            "Epoch 9/50\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 1.3212 - accuracy: 0.5400 - val_loss: 1.2276 - val_accuracy: 0.5300\n",
            "Epoch 10/50\n",
            "4/4 [==============================] - 0s 41ms/step - loss: 1.3060 - accuracy: 0.5000 - val_loss: 1.1723 - val_accuracy: 0.6100\n",
            "Epoch 11/50\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 1.1721 - accuracy: 0.5900 - val_loss: 1.1508 - val_accuracy: 0.5500\n",
            "Epoch 12/50\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 1.1773 - accuracy: 0.5800 - val_loss: 1.0258 - val_accuracy: 0.6400\n",
            "Epoch 13/50\n",
            "4/4 [==============================] - 0s 43ms/step - loss: 0.9742 - accuracy: 0.6500 - val_loss: 1.0462 - val_accuracy: 0.6700\n",
            "Epoch 14/50\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 1.0418 - accuracy: 0.6400 - val_loss: 1.1158 - val_accuracy: 0.5500\n",
            "Epoch 15/50\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.8736 - accuracy: 0.6500 - val_loss: 1.0078 - val_accuracy: 0.6400\n",
            "Epoch 16/50\n",
            "4/4 [==============================] - 0s 42ms/step - loss: 0.8310 - accuracy: 0.6900 - val_loss: 0.9567 - val_accuracy: 0.6300\n",
            "Epoch 17/50\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.8575 - accuracy: 0.6700 - val_loss: 0.9104 - val_accuracy: 0.7000\n",
            "Epoch 18/50\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.7570 - accuracy: 0.7000 - val_loss: 1.0826 - val_accuracy: 0.6100\n",
            "Epoch 19/50\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.8159 - accuracy: 0.6800 - val_loss: 0.9163 - val_accuracy: 0.6700\n",
            "Epoch 20/50\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.7118 - accuracy: 0.7800 - val_loss: 1.0409 - val_accuracy: 0.6300\n",
            "Epoch 21/50\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.7940 - accuracy: 0.7200 - val_loss: 1.0006 - val_accuracy: 0.6300\n",
            "Epoch 22/50\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.7829 - accuracy: 0.6800 - val_loss: 0.8289 - val_accuracy: 0.6900\n",
            "Epoch 23/50\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.7329 - accuracy: 0.7300 - val_loss: 0.8094 - val_accuracy: 0.7000\n",
            "Epoch 24/50\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.6809 - accuracy: 0.7600 - val_loss: 1.0689 - val_accuracy: 0.5900\n",
            "Epoch 25/50\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.7139 - accuracy: 0.6500 - val_loss: 1.0249 - val_accuracy: 0.6300\n",
            "Epoch 26/50\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.6253 - accuracy: 0.7500 - val_loss: 0.8555 - val_accuracy: 0.7100\n",
            "Epoch 27/50\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.6363 - accuracy: 0.7400 - val_loss: 0.9104 - val_accuracy: 0.6700\n",
            "Epoch 28/50\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.6098 - accuracy: 0.7700 - val_loss: 0.8517 - val_accuracy: 0.6700\n",
            "Epoch 29/50\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.6651 - accuracy: 0.7100 - val_loss: 0.8691 - val_accuracy: 0.6900\n",
            "Epoch 30/50\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.6136 - accuracy: 0.7600 - val_loss: 0.8641 - val_accuracy: 0.7200\n",
            "Epoch 31/50\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.5625 - accuracy: 0.8000 - val_loss: 0.9158 - val_accuracy: 0.6800\n",
            "Epoch 32/50\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.5383 - accuracy: 0.7900 - val_loss: 0.8498 - val_accuracy: 0.6800\n",
            "Epoch 33/50\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.5134 - accuracy: 0.8400 - val_loss: 0.7901 - val_accuracy: 0.7100\n",
            "Epoch 34/50\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.5596 - accuracy: 0.7700 - val_loss: 0.7974 - val_accuracy: 0.7000\n",
            "Epoch 35/50\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.6223 - accuracy: 0.7800 - val_loss: 0.9488 - val_accuracy: 0.6700\n",
            "Epoch 36/50\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.5367 - accuracy: 0.8100 - val_loss: 0.9763 - val_accuracy: 0.6900\n",
            "Epoch 37/50\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.6512 - accuracy: 0.7300 - val_loss: 0.9387 - val_accuracy: 0.6900\n",
            "Epoch 38/50\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 0.5121 - accuracy: 0.7600 - val_loss: 0.8921 - val_accuracy: 0.6800\n",
            "Epoch 39/50\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.4360 - accuracy: 0.8400 - val_loss: 0.8442 - val_accuracy: 0.7300\n",
            "Epoch 40/50\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.4726 - accuracy: 0.8300 - val_loss: 0.8448 - val_accuracy: 0.7200\n",
            "Epoch 41/50\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.4452 - accuracy: 0.8500 - val_loss: 0.8528 - val_accuracy: 0.7000\n",
            "Epoch 42/50\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.3982 - accuracy: 0.8500 - val_loss: 0.9120 - val_accuracy: 0.6900\n",
            "Epoch 43/50\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.3870 - accuracy: 0.8600 - val_loss: 0.9134 - val_accuracy: 0.7100\n",
            "Epoch 44/50\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.3290 - accuracy: 0.9100 - val_loss: 0.8404 - val_accuracy: 0.7000\n",
            "Epoch 45/50\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.3873 - accuracy: 0.8600 - val_loss: 0.8638 - val_accuracy: 0.7300\n",
            "Epoch 46/50\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.3462 - accuracy: 0.8600 - val_loss: 0.9255 - val_accuracy: 0.7000\n",
            "Epoch 47/50\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.3311 - accuracy: 0.9100 - val_loss: 0.9751 - val_accuracy: 0.7300\n",
            "Epoch 48/50\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.3382 - accuracy: 0.8900 - val_loss: 0.9951 - val_accuracy: 0.6700\n",
            "Epoch 49/50\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.4370 - accuracy: 0.8200 - val_loss: 0.9634 - val_accuracy: 0.6900\n",
            "Epoch 50/50\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.5057 - accuracy: 0.7700 - val_loss: 1.0510 - val_accuracy: 0.7100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9PSf4qgiQJG",
        "outputId": "1bfe167a-fba6-42c6-fcb8-9daed1688f4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "loss, acc = model.evaluate(test_ds)\n",
        "print(\"Accuracy\", acc)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 555/1869 [=======>......................] - ETA: 18s - loss: 0.8614 - accuracy: 0.7043"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-328175e007f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1504\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_test_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \"\"\"\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_test_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    313\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_test_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1030\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_called_in_fit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_predict_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    510\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \"\"\"\n\u001b[1;32m   1093\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1058\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BkRvvsXb6SI"
      },
      "source": [
        "### Custom data augmentation\n",
        "\n",
        "You can also create custom data augmenation layers. This tutorial shows two ways of doing so. First, you will create a `layers.Lambda` layer. This is a good way to write concise code. Next, you will write a new layer via [subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models), which gives you more control. Both layers will randomly invert the colors in an image, according to some probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMxEhIVXmAH0"
      },
      "source": [
        "def random_invert_img(x, p=0.5):\n",
        "  if  tf.random.uniform([]) < p:\n",
        "    x = (255-x)\n",
        "  else:\n",
        "    x\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0huNpxdmDKu"
      },
      "source": [
        "def random_invert(factor=0.5):\n",
        "  return layers.Lambda(lambda x: random_invert_img(x, factor))\n",
        "\n",
        "random_invert = random_invert()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAcOluP0TNG6"
      },
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(9):\n",
        "  augmented_image = random_invert(image)\n",
        "  ax = plt.subplot(3, 3, i + 1)\n",
        "  plt.imshow(np.reshape(augmented_image[0].numpy().astype(\"uint8\"), (28,28)), cmap='gray')\n",
        "  plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd9XG2PLM5ZJ"
      },
      "source": [
        "Next, implement a custom layer by [subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d11eExc-Ke-7"
      },
      "source": [
        "class RandomInvert(layers.Layer):\n",
        "  def __init__(self, factor=0.5, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.factor = factor\n",
        "\n",
        "  def call(self, x):\n",
        "    return random_invert_img(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX-VQgkRL6fc"
      },
      "source": [
        "_ = plt.imshow(RandomInvert()(image)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0nmllnXZO6T"
      },
      "source": [
        "Both of these layers can be used as described in options 1 and 2 above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7-k__2dAfX6"
      },
      "source": [
        "## Using tf.image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJco2x35EAMs"
      },
      "source": [
        "The above `layers.preprocessing` utilities are convenient. For finer control, you can write your own data augmentation pipelines or layers using `tf.data` and `tf.image`. You may also want to check out [TensorFlow Addons Image: Operations](https://www.tensorflow.org/addons/tutorials/image_ops) and [TensorFlow I/O: Color Space Conversions](https://www.tensorflow.org/io/tutorials/colorspace)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR1RvjYkdd_i"
      },
      "source": [
        "Since the flowers dataset was previously configured with data augmentation, let's reimport it to start fresh."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB-lAS0z9ZJY"
      },
      "source": [
        "(train_ds, val_ds, test_ds), metadata = tfds.load(\n",
        "    'tf_flowers',\n",
        "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ3pqBTS9hNj"
      },
      "source": [
        "Retrieve an image to work with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDsPaAi8de_j"
      },
      "source": [
        "image, label = next(iter(train_ds))\n",
        "_ = plt.imshow(np.reshape(image, (28,28)))\n",
        "_ = plt.title(get_label_name(label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chelxcPtFiTF"
      },
      "source": [
        "Let's use the following function to visualize and compare the original and augmented images side-by-side."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN1ykjJCHikc"
      },
      "source": [
        "def visualize(original, augmented):\n",
        "  fig = plt.figure()\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.title('Original image')\n",
        "  plt.imshow(original)\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.title('Augmented image')\n",
        "  plt.imshow(augmented)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5X4ijQYHmlt"
      },
      "source": [
        "### Data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRD9oujLHo6c"
      },
      "source": [
        "#### Flipping the image\n",
        "\n",
        "Flip the image either vertically or horizontally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZjVI24nIH0S"
      },
      "source": [
        "flipped = tf.image.flip_left_right(image)\n",
        "visualize(image, flipped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iD_lLibIL9q"
      },
      "source": [
        "#### Grayscale the image\n",
        "\n",
        "Grayscale an image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikaMj0guIRtL"
      },
      "source": [
        "grayscaled = tf.image.rgb_to_grayscale(image)\n",
        "visualize(image, tf.squeeze(grayscaled))\n",
        "_ = plt.colorbar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-5yjIs4IZ7v"
      },
      "source": [
        "#### Saturate the image\n",
        "\n",
        "Saturate an image by providing a saturation factor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHz-NosiInmz"
      },
      "source": [
        "saturated = tf.image.adjust_saturation(image, 3)\n",
        "visualize(image, saturated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWXiy8qfIqdC"
      },
      "source": [
        "#### Change image brightness\n",
        "\n",
        "Change the brightness of image by providing a brightness factor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hdG-j46I0nJ"
      },
      "source": [
        "bright = tf.image.adjust_brightness(image, 0.4)\n",
        "visualize(image, bright)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjEOFEITJOr2"
      },
      "source": [
        "#### Center crop the image\n",
        "\n",
        "Crop the image from center up to the image part you desire."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWkK5GFHJUKT"
      },
      "source": [
        "cropped = tf.image.central_crop(image, central_fraction=0.5)\n",
        "visualize(image,cropped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unt76GebI3Gc"
      },
      "source": [
        "#### Rotate the image\n",
        "\n",
        "Rotate an image by 90 degrees."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b19KuAhkJKR-"
      },
      "source": [
        "rotated = tf.image.rot90(image)\n",
        "visualize(image, rotated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CPP0vEKB56X"
      },
      "source": [
        "### Random transformations\n",
        "\n",
        "Warning: There are two sets of random image operations: `tf.image.random*` and `tf.image.stateless_random*`. Using `tf.image.random*` operations is strongly discouraged as they use the old RNGs from TF 1.x. Instead, please use the random image operations introduced in this tutorial. For more information, please refer to [Random number generation](https://www.tensorflow.org/guide/random_numbers).\n",
        "\n",
        "Applying random transformations to the images can further help generalize and expand the dataset. Current `tf.image` API provides 8 such random image operations (ops):\n",
        "\n",
        "*   [`tf.image.stateless_random_brightness`](https://www.tensorflow.org/api_docs/python/tf/image/stateless_random_brightness)\n",
        "*   [`tf.image.stateless_random_contrast`](https://www.tensorflow.org/api_docs/python/tf/image/stateless_random_contrast)\n",
        "*   [`tf.image.stateless_random_crop`](https://www.tensorflow.org/api_docs/python/tf/image/stateless_random_crop)\n",
        "*   [`tf.image.stateless_random_flip_left_right`](https://www.tensorflow.org/api_docs/python/tf/image/stateless_random_flip_left_right)\n",
        "*   [`tf.image.stateless_random_flip_up_down`](https://www.tensorflow.org/api_docs/python/tf/image/stateless_random_flip_up_down)\n",
        "*   [`tf.image.stateless_random_hue`](https://www.tensorflow.org/api_docs/python/tf/image/stateless_random_hue)\n",
        "*   [`tf.image.stateless_random_jpeg_quality`](https://www.tensorflow.org/api_docs/python/tf/image/stateless_random_jpeg_quality)\n",
        "*   [`tf.image.stateless_random_saturation`](https://www.tensorflow.org/api_docs/python/tf/image/stateless_random_saturation)\n",
        "\n",
        "These random image ops are purely functional: the ouput only depends on the input. This makes them simple to use in high performance, deterministic input pipelines. They require a `seed` value be input each step. Given the same `seed`, they return the same results independent of how many times they are called.\n",
        "\n",
        "Note: `seed` is a `Tensor` of shape `(2,)`  whose values are any integers.\n",
        "\n",
        "In the following sections, you will:\n",
        "1.   Go over examples of using random image operations to transform an image; and\n",
        "2.   Demonstrate how to apply random transformations to a training dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "251Wy-MqE4La"
      },
      "source": [
        "#### Randomly change image brightness\n",
        "\n",
        "Randomly change the brightness of `image` by providing a brightness factor and `seed`. The brightness factor is chosen randomly in the range `[-max_delta, max_delta)` and is associated with the given `seed`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fFd1kh7Fr-_"
      },
      "source": [
        "for i in range(3):\n",
        "  seed = (i, 0)  # tuple of size (2,)\n",
        "  stateless_random_brightness = tf.image.stateless_random_brightness(\n",
        "      image, max_delta=0.95, seed=seed)\n",
        "  visualize(image, stateless_random_brightness)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLaDEmooUfYJ"
      },
      "source": [
        "#### Randomly change image contrast\n",
        "\n",
        "Randomly change the contrast of `image` by providing a contrast range and `seed`. The contrast range is chosen randomly in the interval `[lower, upper]` and is associated with the given `seed`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmcYoQHaUoke"
      },
      "source": [
        "for i in range(3):\n",
        "  seed = (i, 0)  # tuple of size (2,)\n",
        "  stateless_random_contrast = tf.image.stateless_random_contrast(\n",
        "      image, lower=0.1, upper=0.9, seed=seed)\n",
        "  visualize(image, stateless_random_contrast)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxb-MP-KVPNz"
      },
      "source": [
        "#### Randomly crop an image\n",
        "\n",
        "Randomly crop `image` by providing target `size` and `seed`. The portion that gets cropped out of `image` is at a randomly chosen offset and is associated with the given `seed`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtZQbUw0VOm5"
      },
      "source": [
        "for i in range(3):\n",
        "  seed = (i, 0)  # tuple of size (2,)\n",
        "  stateless_random_crop = tf.image.stateless_random_crop(\n",
        "      image, size=[210, 300, 3], seed=seed)\n",
        "  visualize(image, stateless_random_crop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isrM-MZtpxTq"
      },
      "source": [
        "### Apply augmentation to a dataset\n",
        "\n",
        "Let's first download the image dataset again in case they are modified in the previous sections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xC80NQP809Uo"
      },
      "source": [
        "(train_datasets, val_ds, test_ds), metadata = tfds.load(\n",
        "    'tf_flowers',\n",
        "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMo9HTDV0Gaz"
      },
      "source": [
        "Let's define a utility function for resizing and rescaling the images. This function will be used in unifying the size and scale of images in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JKmx06lfcFr"
      },
      "source": [
        "def resize_and_rescale(image, label):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
        "  image = (image / 255.0)\n",
        "  return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7OpE_-jWq-I"
      },
      "source": [
        "Let's also define `augment` function that can apply the random transformations to the images. This function will be used on the dataset in the next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KitLdvlpVxPa"
      },
      "source": [
        "def augment(image_label, seed):\n",
        "  image, label = image_label\n",
        "  image, label = resize_and_rescale(image, label)\n",
        "  image = tf.image.resize_with_crop_or_pad(image, IMG_SIZE + 6, IMG_SIZE + 6)\n",
        "  # Make a new seed\n",
        "  new_seed = tf.random.experimental.stateless_split(seed, num=1)[0, :]\n",
        "  # Random crop back to the original size\n",
        "  image = tf.image.stateless_random_crop(\n",
        "      image, size=[IMG_SIZE, IMG_SIZE, 3], seed=seed)\n",
        "  # Random brightness\n",
        "  image = tf.image.stateless_random_brightness(\n",
        "      image, max_delta=0.5, seed=new_seed)\n",
        "  image = tf.clip_by_value(image, 0, 1)\n",
        "  return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlXRsVp70hg8"
      },
      "source": [
        "#### Option 1: Using `tf.data.experimental.Counter()`\n",
        "\n",
        "Create a `tf.data.experimental.Counter()` object (let's call it `counter`) and `zip` the dataset with `(counter, counter)`. This will ensure that each image in the dataset gets associated with a unique value (of shape `(2,)`) based on `counter` which later can get passed into the `augment` function as the `seed` value for random transformations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ6Qq0IWznfi"
      },
      "source": [
        "# Create counter and zip together with train dataset\n",
        "counter = tf.data.experimental.Counter()\n",
        "train_ds = tf.data.Dataset.zip((train_datasets, (counter, counter)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF9ybVQ94X9f"
      },
      "source": [
        "Map the `augment` function to the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQK9BDKk1_3N"
      },
      "source": [
        "train_ds = (\n",
        "    train_ds\n",
        "    .shuffle(1000)\n",
        "    .map(augment, num_parallel_calls=AUTOTUNE)\n",
        "    .batch(batch_size)\n",
        "    .prefetch(AUTOTUNE)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AQoyA-k3ELk"
      },
      "source": [
        "val_ds = (\n",
        "    val_ds\n",
        "    .map(resize_and_rescale, num_parallel_calls=AUTOTUNE)\n",
        "    .batch(batch_size)\n",
        "    .prefetch(AUTOTUNE)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2IQN3NN3G_M"
      },
      "source": [
        "test_ds = (\n",
        "    test_ds\n",
        "    .map(resize_and_rescale, num_parallel_calls=AUTOTUNE)\n",
        "    .batch(batch_size)\n",
        "    .prefetch(AUTOTUNE)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvTVY8BY2LpD"
      },
      "source": [
        "#### Option 2: Using `tf.random.Generator`\n",
        "\n",
        "Create a `tf.random.Generator` object with an intial `seed` value. Calling `make_seeds` function on the same generator object returns a new, unique `seed` value always. Define a wrapper function that 1) calls `make_seeds` function and that 2) passes the newly generated `seed` value into the `augment` function for random transformations.\n",
        "\n",
        "Note: `tf.random.Generator` objects store RNG state in a `tf.Variable`, which means it can be saved as a [checkpoint](https://www.tensorflow.org/guide/checkpoint) or in a [SavedModel](https://www.tensorflow.org/guide/saved_model). For more details, please refer to [Random number generation](https://www.tensorflow.org/guide/random_numbers)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQDvedZ33eAy"
      },
      "source": [
        "# Create a generator\n",
        "rng = tf.random.Generator.from_seed(123, alg='philox')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDEkO1nt2ta0"
      },
      "source": [
        "# A wrapper function for updating seeds\n",
        "def f(x, y):\n",
        "  seed = rng.make_seeds(2)[0]\n",
        "  image, label = augment((x, y), seed)\n",
        "  return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyPC4vUM4MT0"
      },
      "source": [
        "Map the wrapper function `f` to the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu2uB7k12xKw"
      },
      "source": [
        "train_ds = (\n",
        "    train_datasets\n",
        "    .shuffle(1000)\n",
        "    .map(f, num_parallel_calls=AUTOTUNE)\n",
        "    .batch(batch_size)\n",
        "    .prefetch(AUTOTUNE)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6caldPi2HAP"
      },
      "source": [
        "val_ds = (\n",
        "    val_ds\n",
        "    .map(resize_and_rescale, num_parallel_calls=AUTOTUNE)\n",
        "    .batch(batch_size)\n",
        "    .prefetch(AUTOTUNE)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceaCdJnh2I-r"
      },
      "source": [
        "test_ds = (\n",
        "    test_ds\n",
        "    .map(resize_and_rescale, num_parallel_calls=AUTOTUNE)\n",
        "    .batch(batch_size)\n",
        "    .prefetch(AUTOTUNE)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKwCA6AOjTrc"
      },
      "source": [
        "These datasets can now be used to train a model as shown previously."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YypDihDlj0no"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "This tutorial demonstrated data augmentation using [Keras Preprocessing Layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/) and `tf.image`. To learn how to include preprocessing layers inside your model, see the [Image classification](https://www.tensorflow.org/tutorials/images/classification) tutorial. You may also be interested in learning how preprocessing layers can help you classify text, as shown in the [Basic text classification](https://www.tensorflow.org/tutorials/keras/text_classification) tutorial. You can learn more about `tf.data` in this [guide](https://www.tensorflow.org/guide/data), and you can learn how to configure your input pipelines for performance [here](https://www.tensorflow.org/guide/data_performance)."
      ]
    }
  ]
}