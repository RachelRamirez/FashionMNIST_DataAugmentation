{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion mnist_convnet",
      "provenance": [],
      "collapsed_sections": [
        "3tLeuxPfOUwh",
        "vqMnsJfoOUwm",
        "R5UJ4XGxOUwp",
        "j3J-NWLSOUwr"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RachelRamirez/FashionMNIST_DataAugmentation/blob/main/Fashion_mnist_convnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tLeuxPfOUwh"
      },
      "source": [
        "# \"Simple MNIST convnet\" Architecture with Extremely Limited Data used to Train CNN\n",
        "\n",
        "**Original Author as applied to MNIST (Numbers):** [fchollet](https://twitter.com/fchollet)<br>\n",
        "**Date created:** 2015/06/19<br>\n",
        "**Last modified:** 2020/04/21<br>\n",
        "**Applied to Fashion MNIST** 2021/08/25\n",
        "\n",
        "**Description:** A simple convnet that achieves ~90% test accuracy on MNIST, is applied to the Fashion MNIST.\n",
        "\n",
        "Then data is limited to 1000 training samples to view the effects of data augmentation on increasing model accuracy.\n",
        "\n",
        "More features are added for residual/error analysis such as confusion matrix and data augmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpKcttwnrTnT"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJfFyadErbBf"
      },
      "source": [
        "The first part of this experiment is looking at the space for TrainingSize, ValidationSize, BatchSize, and EpochSize.  These all likely effect one another (40way interactions) so a screening design is used to look for maineffects.\n",
        "\n",
        "A: BatchSize\n",
        "\n",
        "B: Epochs\n",
        "\n",
        "C: Training Size (Count)\n",
        "\n",
        "D: Validation (as percentage of Training Size)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqEfqqnXr9Ji"
      },
      "source": [
        "## Experiment 1 Variables\n",
        "\n",
        "var_BatchSize =  10,  100\n",
        "var_Epochs =     15,  50\n",
        "var_TrainSize = 100,  1000\n",
        "var_ValPercent = 0.2, 0.5\n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqMnsJfoOUwm"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGvYofsKOUwo"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5UJ4XGxOUwp"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW6G-5CNOUwq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d0a71e7-7f84-4628-badf-0b97b9851557"
      },
      "source": [
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "x_train = x_train[0:1000]\n",
        "y_train = y_train[0:1000]\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")\n",
        "\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (1000, 28, 28, 1)\n",
            "1000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3J-NWLSOUwr"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MEHlGkLOUwt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ade927b6-0235-4962-f04e-1e0eaff58549"
      },
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                16010     \n",
            "=================================================================\n",
            "Total params: 34,826\n",
            "Trainable params: 34,826\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mLvi5vnOUww"
      },
      "source": [
        "# Insert DOE Variables -> Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "rJbHvwogyjG3",
        "outputId": "c36c80f7-d06d-4ac3-ddbc-b07b0250a5fe"
      },
      "source": [
        "# scorelist = []\n",
        "# for b in var_BatchSize:\n",
        "#   for e in var_Epochs:\n",
        "#      for t in var_TrainSize:\n",
        "#        for v in var_ValPercent:\n",
        "#           scorelist += [[b,e,t,v, 100, \"   \" ]]\n",
        "\n",
        "\n",
        "\n",
        "# display(scorelist)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[[10, 15, 100, 0.2, 100, '   '],\n",
              " [10, 15, 100, 0.5, 100, '   '],\n",
              " [10, 15, 1000, 0.2, 100, '   '],\n",
              " [10, 15, 1000, 0.5, 100, '   '],\n",
              " [10, 50, 100, 0.2, 100, '   '],\n",
              " [10, 50, 100, 0.5, 100, '   '],\n",
              " [10, 50, 1000, 0.2, 100, '   '],\n",
              " [10, 50, 1000, 0.5, 100, '   '],\n",
              " [100, 15, 100, 0.2, 100, '   '],\n",
              " [100, 15, 100, 0.5, 100, '   '],\n",
              " [100, 15, 1000, 0.2, 100, '   '],\n",
              " [100, 15, 1000, 0.5, 100, '   '],\n",
              " [100, 50, 100, 0.2, 100, '   '],\n",
              " [100, 50, 100, 0.5, 100, '   '],\n",
              " [100, 50, 1000, 0.2, 100, '   '],\n",
              " [100, 50, 1000, 0.5, 100, '   ']]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UbYwOVJv7GQ"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gfBEyf9v7GS",
        "outputId": "4ba57e0a-5393-4fd8-a73e-4d77dec41cc8"
      },
      "source": [
        "counter = 1\n",
        "scorelist = []\n",
        "\n",
        "for b in var_BatchSize:\n",
        "  for e in var_Epochs:\n",
        "     for t in var_TrainSize:\n",
        "       for v in var_ValPercent:\n",
        "        \n",
        "        batch_size = b\n",
        "        epochs = e\n",
        "        x_train = x_train[0:t]\n",
        "        y_train = y_train[0:t]\n",
        "\n",
        "        model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "        model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=v, verbose=0)\n",
        "\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        # print(\"Test loss:\", score[0])\n",
        "        # print(\"Test accuracy:\", score[1])\n",
        "        scorelist += [[counter, b,  e,   t,   v,   score[0], score[1] ]]\n",
        "        print(\"This is run\", counter, \": \", b, e, t, v,  \"had accuracy: \", score[1], \" test loss: \", score[0])\n",
        "        counter=counter+1"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is run 1 :  10 15 100 0.2 had accuracy:  0.6973000168800354  test loss:  1.6851215362548828\n",
            "This is run 2 :  10 15 100 0.5 had accuracy:  0.713699996471405  test loss:  1.832808256149292\n",
            "This is run 3 :  10 15 1000 0.2 had accuracy:  0.7027999758720398  test loss:  1.855257272720337\n",
            "This is run 4 :  10 15 1000 0.5 had accuracy:  0.7038000226020813  test loss:  2.1999452114105225\n",
            "This is run 5 :  10 50 100 0.2 had accuracy:  0.6995000243186951  test loss:  2.167445182800293\n",
            "This is run 6 :  10 50 100 0.5 had accuracy:  0.7178000211715698  test loss:  2.3670055866241455\n",
            "This is run 7 :  10 50 1000 0.2 had accuracy:  0.6883000135421753  test loss:  2.628786087036133\n",
            "This is run 8 :  10 50 1000 0.5 had accuracy:  0.7113999724388123  test loss:  3.0950543880462646\n",
            "This is run 9 :  100 15 100 0.2 had accuracy:  0.692799985408783  test loss:  2.929530382156372\n",
            "This is run 10 :  100 15 100 0.5 had accuracy:  0.6934000253677368  test loss:  3.111909866333008\n",
            "This is run 11 :  100 15 1000 0.2 had accuracy:  0.7042999863624573  test loss:  2.827313184738159\n",
            "This is run 12 :  100 15 1000 0.5 had accuracy:  0.7084000110626221  test loss:  2.95511531829834\n",
            "This is run 13 :  100 50 100 0.2 had accuracy:  0.7084000110626221  test loss:  2.866485357284546\n",
            "This is run 14 :  100 50 100 0.5 had accuracy:  0.7129999995231628  test loss:  3.1511518955230713\n",
            "This is run 15 :  100 50 1000 0.2 had accuracy:  0.7142999768257141  test loss:  2.792529582977295\n",
            "This is run 16 :  100 50 1000 0.5 had accuracy:  0.7006999850273132  test loss:  3.2242579460144043\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "wwGJ493w15x9",
        "outputId": "e9444356-25fc-4774-fb29-af203c613ea2"
      },
      "source": [
        "domains.to_csv(\"path_to_local_git_folder/domains.csv\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-8f154cce300b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdomains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path_to_local_git_folder/domains.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'domains' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44kD7Vn6OUwy"
      },
      "source": [
        "# batch_size = 50\n",
        "# epochs = 30\n",
        "\n",
        "# model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_OXslKCOUw0"
      },
      "source": [
        "## Evaluate the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BD9T7nz9OUw1"
      },
      "source": [
        "          score = model.evaluate(x_test, y_test, verbose=1)\n",
        "          print(\"Test loss:\", score[0])\n",
        "          print(\"Test accuracy:\", score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyqgW4Qyn3y5"
      },
      "source": [
        "predictions = model.predict(x_test)\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        " \n",
        "y_test = np.argmax(y_test, axis=1) \n",
        "\n",
        "confusion_matrix = tf.math.confusion_matrix(y_test, predictions)  #First Variable is on VERTICAL, second Variable is on X HORIZONTAL\n",
        "#confusion_matrix = tf.math.confusion_matrix(predictions, tf.Variable(np.ones(predictions.shape)))\n",
        "\n",
        "f, ax = plt.subplots(figsize=(9, 7))\n",
        "sn.heatmap(\n",
        "    confusion_matrix,\n",
        "    annot=True,\n",
        "    linewidths=.5,\n",
        "    fmt=\"d\",\n",
        "    square=True,\n",
        "    ax=ax\n",
        ")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YobOk8sNTgip"
      },
      "source": [
        "In my first run, no data augmentation, 100 random samples of the Training Set, using 50% of that as the Validation Set, using a Batch Size of 50 and Epochs = 30, the resulting accuracy of the test-set is Test loss: 0.912209689617157\n",
        "Test accuracy: 0.6866999864578247"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aRFTHeG1SkQ"
      },
      "source": [
        "## I want to pick the largest value on the confusion matrix not on the diagonal\n",
        "confusing_part_matrix= np.array(confusion_matrix)-np.identity(confusion_matrix.shape[0])*np.diag(confusion_matrix)\n",
        "\n",
        "confusing_part_matrix = tf.convert_to_tensor(confusing_part_matrix)\n",
        "\n",
        "f, ax = plt.subplots(figsize=(9, 7))\n",
        "sn.heatmap(\n",
        "    confusing_part_matrix,\n",
        "    annot=True,\n",
        "    linewidths=.5\n",
        "    #,fmt=\"d\"\n",
        "    #,square=True\n",
        "    #,ax=ax\n",
        ")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "## find the max value of those remaining numbers\n",
        "thisnumber=np.max(confusing_part_matrix)\n",
        "\n",
        "x_thisnumber=np.argmax(confusing_part_matrix,axis=0)\n",
        "y_thisnumber=np.argmax(confusing_part_matrix,axis=1)\n",
        "z_thisnumber=np.argmax(confusing_part_matrix)\n",
        "display(x_thisnumber,y_thisnumber)\n",
        "#print(\"The worst the algorithm did is between \" confus)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT4qB3nZ2WwB"
      },
      "source": [
        "confusing_part_matrix[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo0qd0Wy5jmT"
      },
      "source": [
        "z_thisnumber=np.argmax(confusing_part_matrix)\n",
        "z_thisnumber\n",
        "print(\"So the most confused classes were between: \" , np.math.floor(z_thisnumber/10), \" a\",  LABEL_NAMES[np.math.floor(z_thisnumber/10)] , \" and  \", z_thisnumber%10, \" a \",  \n",
        "LABEL_NAMES[z_thisnumber%10])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhozCsi1MjMD"
      },
      "source": [
        "\n",
        "LABEL_NAMES = ['t_shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle_boots']\n",
        "\n",
        "delta = predictions - y_test\n",
        "\n",
        "wrong = tf.boolean_mask(predictions, delta)\n",
        "print(wrong)\n",
        "\n",
        "#  if i != 0\n",
        "#    display(\"Prediction \", i, \" is \" + LABEL_NAMES[predictions[i]], \"but it is \",  LABEL_NAMES[y_test[i] )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3V0jrX_Mmxl"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K24O6nCMnPP"
      },
      "source": [
        "plt.imshow(x_test[0].reshape((28,28)), cmap=plt.cm.binary)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}