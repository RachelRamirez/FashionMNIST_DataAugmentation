{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion mnist_convnet",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RachelRamirez/FashionMNIST_DataAugmentation/blob/main/Fashion_mnist_convnet_augmentingdata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tLeuxPfOUwh"
      },
      "source": [
        "# \"Simple (Fashion) MNIST convnet\" Architecture with Extremely Limited Data used to Train CNN\n",
        "\n",
        "**Original Author with MNIST:** [fchollet](https://twitter.com/fchollet)<br>\n",
        "Date created: 2015/06/19<br>\n",
        "\n",
        "**Modified to Fashion MNIST** [rachelramirez](https://github.com/RachelRamirez/FashionMNIST_DataAugmentation) 2021/08/25\n",
        "\n",
        "**Description:** A simple convnet architecture for training on Fashion MNIST, is frozen and then data augmentation techniques are attempted as a Design of Experiment (DOE).\n",
        "\n",
        "Then data is limited to varying levels of sample sizes  [500, 1000]  to view the effect of data augmentation on increasing model accuracy.\n",
        "\n",
        "More features are added to original notebook for residual/error analysis such as confusion matrix and data augmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3haQSkOOYVl",
        "outputId": "4667c145-32e0-45d9-f981-70b3a1547428"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Sep  1 22:21:52 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.57.02    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGvYofsKOUwo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "170014e1-774e-49f4-86fd-6f2f226022ba"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "from google.colab import files\n",
        "from keras.layers import GaussianNoise\n",
        "\n",
        "\n",
        "print(tf. __version__) \n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpKcttwnrTnT"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJfFyadErbBf"
      },
      "source": [
        "The first part of this experiment is looking at the space for *TrainingSize*, *ValidationSize*, *BatchSize*, and *EpochSize*.  These all likely effect one another (4-way interactions) so a DOE design is used to look at Response Surface Methods.\n",
        "\n",
        "A: BatchSize (10, 100)\n",
        "\n",
        "B: Epochs  (15, 50)\n",
        "\n",
        "C: Training Size (Count)  (100, 1000)\n",
        "\n",
        "D: Validation (as percentage of Training Size)  (20%, 50%) \n",
        "\n",
        "Replicates: 2\n",
        "Center Points: 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqEfqqnXr9Ji"
      },
      "source": [
        "\n",
        " "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5UJ4XGxOUwp"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW6G-5CNOUwq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65dcc6ae-8c8f-4566-b9f1-3f75af94c1e1"
      },
      "source": [
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Until the end of all analysis I don't really want to look at the real test set results\n",
        "# I'm overwriting them to make sure I don't accidentally use them\n",
        "\n",
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# I decide to take a small dataset - 2000 samples of the 60000, and use that to \n",
        "# create a TRAIN Set of up to 1000 samples, a VALIDATION Set of up to 1000 samples, and a FAKE-TEST set of the remaining 58000\n",
        "# After Analyzing the best \"Augmentation Effect\" I wil apply it to the \"REAL\" Test DataSet to see if it improved.\n",
        "\n",
        "\n",
        "# Validation Set is 1000 \n",
        "x_val = x_train[1000:2000]\n",
        "y_val = y_train[1000:2000]\n",
        "\n",
        "#My FAKE Test Set is 58000\n",
        "x_test = x_train[2000:60000]\n",
        "y_test = y_train[2000:60000]\n",
        "\n",
        "\n",
        "# Finally I change train dataset to first 1000\n",
        "x_train = x_train[0:1000]\n",
        "y_train = y_train[0:1000]\n",
        "\n",
        "#My holdout Test Set after all the DOE is the original TEST set from dataset\n",
        "\n",
        "\n",
        "print(\"Fashion MNIST has training size up to: \", y_train.shape, \", val set of , \", y_val.shape, \" and test set of \", y_test.shape)\n",
        "\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "#x_train = x_train.astype(\"float32\") / 255\n",
        "#x_test = x_test.astype(\"float32\") / 255    # put into preprocessing model step\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "x_val = np.expand_dims(x_val, -1)\n",
        "\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_val.shape[0], \"validation samples\")\n",
        "print(x_test.shape[0], \"fake test samples saved from test-set\")\n",
        "\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Try to put in tensor flow shape\n",
        "x_val = tf.stack(x_val)\n",
        "y_val = tf.stack(y_val)\n",
        "\n",
        "\n",
        "# Define Manual Validation Set\n",
        "valid_set = (x_val, y_val)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n",
            "Fashion MNIST has training size up to:  (1000,) , val set of ,  (1000,)  and test set of  (58000,)\n",
            "x_train shape: (1000, 28, 28, 1)\n",
            "1000 train samples\n",
            "1000 validation samples\n",
            "58000 fake test samples saved from test-set\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmLbFwrtYy7h"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3J-NWLSOUwr"
      },
      "source": [
        "## DOE Loops with Model Building "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftmEQF7fjEBg"
      },
      "source": [
        "# ## Scratch try first\n",
        "\n",
        "var_flip = 0, 1           #0 means no flip, 1 means horizontal flip\n",
        "var_contrast = 0, 0.2     # \"\"              0.2 means \n",
        "var_rotate = 0, 0.5\n",
        "var_noise = 0, 0.1\n",
        "\n",
        "name_flip = \"NoFlip\", \"Horizontal\"\n",
        "name_contrast = \"0contrast\", \"0.2contrast\"\n",
        "name_rotate = \"0rotate\", \"0.5rotate\"\n",
        "name_noise = \"0noise\", \"0.1noise\"\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MEHlGkLOUwt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cfd93e3-a672-46e0-e908-3c61538092d5"
      },
      "source": [
        "dataAugmentation = True\n",
        "randseed = 42 \n",
        "\n",
        "\n",
        "flip_layer = layers.experimental.preprocessing.RandomFlip(\"horizontal\", seed = randseed, name = \"FlipHorizontal\")\n",
        "rotate_layer = layers.experimental.preprocessing.RandomRotation(0, seed = randseed, name = \"Rotate\")\n",
        "contrast_layer = layers.experimental.preprocessing.RandomContrast(factor=1, seed = randseed, name = \"Contrast\") \n",
        "noise_layer = layers.GaussianNoise(0.1, name = \"GaussianNoise\")\n",
        "\n",
        "augment_flip = False\n",
        "if augment_flip == False:\n",
        "  data_augmentation = tf.keras.Sequential([\n",
        "      rotate_layer,  \n",
        "      contrast_layer, \n",
        "      noise_layer \n",
        "  ])\n",
        "else: \n",
        "    data_augmentation = tf.keras.Sequential([\n",
        "      flip_layer,  \n",
        "      rotate_layer,  \n",
        "      contrast_layer, \n",
        "      noise_layer \n",
        "  ])\n",
        "\n",
        "\n",
        "if dataAugmentation == False:\n",
        "  model = keras.Sequential(\n",
        "      [\n",
        "          keras.Input(shape=input_shape),\n",
        "          layers.experimental.preprocessing.Rescaling(1./255, input_shape=(input_shape)),\n",
        "          layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "          layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "          layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "          layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "          layers.Flatten(),\n",
        "          layers.Dropout(0.5),\n",
        "          layers.Dense(num_classes, activation=\"softmax\"),\n",
        "      ]\n",
        "  )\n",
        "else:\n",
        "    model = keras.Sequential(\n",
        "      [\n",
        "          keras.Input(shape=input_shape),\n",
        "          layers.experimental.preprocessing.Rescaling(1./255, input_shape=(input_shape)),\n",
        "          data_augmentation,   \n",
        "          layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "          layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "          layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "          layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "          layers.Flatten(),\n",
        "          layers.Dropout(0.5),\n",
        "          layers.Dense(num_classes, activation=\"softmax\"),\n",
        "      ]\n",
        "  )\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "rescaling (Rescaling)        (None, 28, 28, 1)         0         \n",
            "_________________________________________________________________\n",
            "sequential (Sequential)      (None, 28, 28, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                16010     \n",
            "=================================================================\n",
            "Total params: 34,826\n",
            "Trainable params: 34,826\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mLvi5vnOUww"
      },
      "source": [
        "# Insert DOE Variables -> Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJbHvwogyjG3"
      },
      "source": [
        "# screen = False\n",
        "\n",
        "# ## Experiment 1 'Global' Variables\n",
        "# if screen == True: \n",
        "#   var_BatchSize =  50, 50 #  # Just do 50\n",
        "#   var_Epochs =     70, 70     # Just do 70\n",
        "#   var_TrainSize = 100, 505, 1000\n",
        "#   #var_ValPercent = 0.2, 0.3\n",
        "# else:\n",
        "#   var_BatchSize =  50, 50   #  # Just do 50\n",
        "#   var_Epochs =     70, 70    # Just do 70\n",
        "#   var_TrainSize = 100, 100\n",
        "#   #var_ValPercent = 0.2, 0.2\n",
        "\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UbYwOVJv7GQ"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "_gfBEyf9v7GS",
        "outputId": "555f26b1-b314-4908-844f-30a8d580a69c"
      },
      "source": [
        "counter = 1\n",
        "scorelist = []\n",
        "\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "## Save Weight is to help Reset the model before Training, very important, otherwise you get better results every run!\n",
        "model.save_weights('model_clear.h5') \n",
        "\n",
        "\n",
        "\n",
        "for t in var_TrainSize:\n",
        "  new_x_train = x_train[0:t]  \n",
        "  new_y_train = y_train[0:t]\n",
        "\n",
        "\n",
        "  for b in var_BatchSize:\n",
        "    for e in var_Epochs:\n",
        "      epochs = e\n",
        "\n",
        "\n",
        "      model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "      model.load_weights('model_clear.h5')\n",
        "\n",
        "      # Verbose  = 0 after I verified everything was working to improve overall speed\n",
        "      %%timeit\n",
        "      model.fit(new_x_train, new_y_train, batch_size=b, epochs=e,  validation_data=valid_set, verbose=0)\n",
        "\n",
        "      score = model.evaluate(x_test, y_test, verbose=0)\n",
        "      # print(\"Test loss:\", score[0])\n",
        "      # print(\"Test accuracy:\", score[1])\n",
        "      scorelist += [ [counter, b,  e,    t,    score[0], score[1]] ]\n",
        "      print(\"Run: \", counter, \"\", b, e, t,  \"Accuracy: \", score[1], \" Loss: \", score[0])\n",
        "      counter=counter+1\n",
        "\n",
        "#save scorelist as a csv file\n",
        "if dataAugmentation:\n",
        "  filename = 'data_augmented.csv'\n",
        "else:\n",
        "    filename = \"data.csv\"\n",
        "\n",
        "np.savetxt(filename, scorelist, delimiter=',')\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-7a18d1b3cbf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_TrainSize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mnew_x_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mnew_y_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'var_TrainSize' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwGJ493w15x9"
      },
      "source": [
        "#save file to my personal computer so I don't lose it to google colab timeout\n",
        "\n",
        "#why isn't this working?! :-( \n",
        "#files.download('data.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44kD7Vn6OUwy"
      },
      "source": [
        "# batch_size = 50\n",
        "# epochs = 30\n",
        "\n",
        "# model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_OXslKCOUw0"
      },
      "source": [
        "## Evaluate the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BD9T7nz9OUw1"
      },
      "source": [
        "          # score = model.evaluate(x_test, y_test, verbose=1)\n",
        "          # print(\"Test loss:\", score[0])\n",
        "          # print(\"Test accuracy:\", score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyqgW4Qyn3y5"
      },
      "source": [
        "predictions = model.predict(x_test)\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        " \n",
        "y_test = np.argmax(y_test, axis=1) \n",
        "\n",
        "confusion_matrix = tf.math.confusion_matrix(y_test, predictions)  #First Variable is on VERTICAL, second Variable is on X HORIZONTAL\n",
        "#confusion_matrix = tf.math.confusion_matrix(predictions, tf.Variable(np.ones(predictions.shape)))\n",
        "\n",
        "f, ax = plt.subplots(figsize=(9, 7))\n",
        "sn.heatmap(\n",
        "    confusion_matrix,\n",
        "    annot=True,\n",
        "    linewidths=.5,\n",
        "    fmt=\"d\",\n",
        "    square=True,\n",
        "    ax=ax\n",
        ")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YobOk8sNTgip"
      },
      "source": [
        "In my first run, no data augmentation, 100 random samples of the Training Set, using 50% of that as the Validation Set, using a Batch Size of 50 and Epochs = 30, the resulting accuracy of the test-set is Test loss: 0.912209689617157\n",
        "Test accuracy: 0.6866999864578247"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aRFTHeG1SkQ"
      },
      "source": [
        "## I want to pick the largest value on the confusion matrix not on the diagonal\n",
        "confusing_part_matrix= np.array(confusion_matrix)-np.identity(confusion_matrix.shape[0])*np.diag(confusion_matrix)\n",
        "\n",
        "confusing_part_matrix = tf.convert_to_tensor(confusing_part_matrix)\n",
        "\n",
        "f, ax = plt.subplots(figsize=(9, 7))\n",
        "sn.heatmap(\n",
        "    confusing_part_matrix,\n",
        "    annot=True,\n",
        "    linewidths=.5\n",
        "    ,fmt='.0f'\n",
        "    #,fmt=\"d\"\n",
        "    #,square=True\n",
        "    #,ax=ax\n",
        ")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "## find the max value of those remaining numbers\n",
        "thisnumber=np.max(confusing_part_matrix)\n",
        "\n",
        "x_thisnumber=np.argmax(confusing_part_matrix,axis=0)\n",
        "y_thisnumber=np.argmax(confusing_part_matrix,axis=1)\n",
        "z_thisnumber=np.argmax(confusing_part_matrix)\n",
        "display(x_thisnumber,y_thisnumber)\n",
        "#print(\"The worst the algorithm did is between \" confus)\n",
        "\n",
        "display(sum(sum(confusing_part_matrix)))\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhozCsi1MjMD"
      },
      "source": [
        "delta = predictions - y_test\n",
        "\n",
        "wrong = tf.boolean_mask(x_test, delta)\n",
        "\n",
        "#  if i != 0\n",
        "#    display(\"Prediction \", i, \" is \" + LABEL_NAMES[predictions[i]], \"but it is \",  LABEL_NAMES[y_test[i] )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT4qB3nZ2WwB"
      },
      "source": [
        "confusing_part_matrix[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo0qd0Wy5jmT"
      },
      "source": [
        "LABEL_NAMES = ['t_shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle_boots']\n",
        "z_thisnumber=np.argmax(confusing_part_matrix)\n",
        "z_thisnumber\n",
        "print(\"The algorithm misrecognizes a lot of class \" ,  LABEL_NAMES[np.math.floor(z_thisnumber/10)] , \" as class \",  LABEL_NAMES[z_thisnumber%10])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3V0jrX_Mmxl"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K24O6nCMnPP"
      },
      "source": [
        "plt.imshow(x_test[0].reshape((28,28)), cmap=plt.cm.binary)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43qaLtBn1JJX"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}